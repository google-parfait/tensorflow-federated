{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "grBmytrShbUE"
      },
      "source": [
        "# High-performance simulations with TFF\n",
        "\n",
        "This tutorial will describe how to setup high-performance simulations with TFF\n",
        "in a variety of common scenarios.\n",
        "\n",
        "NOTE: The mechanisms covered here are not included in the latest release, have\n",
        "not been tested yet, and the API may evolve. In order to follow this tutorial,\n",
        "you will need to build a TFF pip package from scratch from the latest sources, and install it in a Jupyter notebook with a Python 3 runtime. The new executor\n",
        "stack is not compatible with Python 2.\n",
        "\n",
        "TODO(b/134543154): Populate the content, some of the things to cover here:\n",
        "- using GPUs in a single-machine setup,\n",
        "- multi-machine setup on GCP/GKE, with and without TPUs,\n",
        "- interfacing MapReduce-like backends,\n",
        "- current limitations and when/how they will be relaxed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yiq_MY4LopET"
      },
      "source": [
        "## Before we begin\n",
        "\n",
        "First, make sure your notebook is connected to a backend that has the relevant\n",
        "components (including gRPC dependencies for multi-machine scenarios) compiled.\n",
        "\n",
        "Now, if you are running this notebook in Jupyter, you may need to take an extra\n",
        "step to work around the\n",
        "[limitations of Jypter with asyncio](https://github.com/jupyter/notebook/issues/3397#issuecomment-419386811)\n",
        "by installing the [nest_asyncio](https://github.com/erdewit/nest_asyncio)\n",
        "package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tnngjncsPq15"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_zFenI3IPpgI"
      },
      "source": [
        "Now, let's start by loading the MNIST example from the TFF website, and\n",
        "declaring the Python function that will run a small experiment loop. In order\n",
        "to use all data and make sure there is enough work for each round, we partition\n",
        "data from all users into 10 groups and assign one group per simulated client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ke7EyuvG0Zyn"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_federated\n",
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2dVPgxN0MdG2"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow_federated.python.examples import mnist\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 100\n",
        "NUM_CLIENTS = 10\n",
        "\n",
        "source, _ = tff.simulation.datasets.emnist.load_data()\n",
        "\n",
        "def client_data(n):\n",
        "  ids_per_client = int(len(source.client_ids) / NUM_CLIENTS)\n",
        "  start = ids_per_client * n\n",
        "  limit = ids_per_client * (n + 1)\n",
        "  combined_dataset = None\n",
        "  while start \u003c limit:\n",
        "    dataset = mnist.keras_dataset_from_emnist(\n",
        "        source.create_tf_dataset_for_client(source.client_ids[start]))\n",
        "    start = start + 1\n",
        "    if combined_dataset is not None:\n",
        "      combined_dataset = combined_dataset.concatenate(dataset)\n",
        "    else:\n",
        "      combined_dataset = dataset\n",
        "  return combined_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
        "\n",
        "train_data = [client_data(n) for n in range(NUM_CLIENTS)]\n",
        "\n",
        "batch = tf.nest.map_structure(lambda x: x.numpy(), iter(train_data[0]).next())\n",
        "\n",
        "def model_fn():\n",
        "  return tff.learning.from_compiled_keras_model(\n",
        "      mnist.create_simple_keras_model(), batch)\n",
        "\n",
        "trainer = tff.learning.build_federated_averaging_process(model_fn)\n",
        "\n",
        "def evaluate(num_rounds=10):\n",
        "  state = trainer.initialize()\n",
        "  for _ in range(num_rounds):\n",
        "    t1 = time.time()\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    t2 = time.time()\n",
        "    print('loss {}, round time {}'.format(str(metrics.loss), str(t2 - t1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CDHJF7EIiEy-"
      },
      "source": [
        "## Single-machine simulations\n",
        "\n",
        "A simple local multi-threaded executor can be created using a new currently\n",
        "undocumented framework function `tff.framework.create_local_executor()`, and\n",
        "made default by calling `tff.framework.set_default_executor()`, as follows.\n",
        "Note that the elocal executor currently requires the number of clients to be\n",
        "specified at setup time. We'll relax this restriction in the near future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "height": 185
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1645275,
          "status": "ok",
          "timestamp": 1562299880040,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "-V6uCS_BMoR9",
        "outputId": "a7ac4aa1-663a-4f1e-8fd1-ef8a6c0eb404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss 3.55805, round time 168.0329692363739\n",
            "loss 1.20457, round time 162.5828468799591\n",
            "loss 0.968885, round time 162.3564817905426\n",
            "loss 0.876231, round time 161.66547751426697\n",
            "loss 0.822056, round time 165.8971517086029\n",
            "loss 0.782698, round time 165.80716514587402\n",
            "loss 0.754549, round time 167.1305811405182\n",
            "loss 0.735254, round time 162.5927665233612\n",
            "loss 0.718297, round time 163.8291881084442\n",
            "loss 0.703123, round time 164.844788312912\n"
          ]
        }
      ],
      "source": [
        "tff.framework.set_default_executor(\n",
        "    tff.framework.create_local_executor(num_clients=NUM_CLIENTS))\n",
        "\n",
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6NnVUd6qM6h-"
      },
      "source": [
        "Now, for comparison, let's run the same training code using the reference\n",
        "executor. The reference executor can be automatically installed back by calling\n",
        "the `tff.framework.set_default_executor()` function without an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "height": 185
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 2815528,
          "status": "ok",
          "timestamp": 1562302695625,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "ThVuHtUVM9LK",
        "outputId": "2ed94546-0cd4-4330-cf10-4bbf45917999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss 3.55805, round time 288.4371428489685\n",
            "loss 1.20457, round time 285.4338390827179\n",
            "loss 0.968885, round time 279.578590631485\n",
            "loss 0.876231, round time 281.2864832878113\n",
            "loss 0.822056, round time 278.5055932998657\n",
            "loss 0.782698, round time 280.6443078517914\n",
            "loss 0.754549, round time 280.1886131763458\n",
            "loss 0.735254, round time 280.347163438797\n",
            "loss 0.718297, round time 280.31703782081604\n",
            "loss 0.703123, round time 280.32683181762695\n"
          ]
        }
      ],
      "source": [
        "tff.framework.set_default_executor()\n",
        "\n",
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hn5NYppqNKH8"
      },
      "source": [
        "One thing to notice is that whereas the new executor affords a higher resource\n",
        "utilization (you should see load spread across multiple CPUs), it is fresh off\n",
        "the oven, and not yet optimized. Consequently, the actual speedup achieved here is about 2x lower than what one might expect simply by comparing CPU\n",
        "utilization between the two setups. Eliminating the sequential inefficiency in\n",
        "the eager executor stack is a work in progress. Stay tuned for updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bZ171NhcNa3M"
      },
      "source": [
        "## Multi-machine simulations on GCP/GKE, GPUs, TPUs, and beyond...\n",
        "\n",
        "Coming very soon."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "High-performance simulations with TFF",
      "provenance": [
        {
          "file_id": "14vSn6H8hu35BMb48b48hHYJ3Lln3OTQL",
          "timestamp": 1561680139142
        }
      ],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
