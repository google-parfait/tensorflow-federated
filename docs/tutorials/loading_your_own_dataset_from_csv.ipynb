{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "loading_your_own_dataset_from_csv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf7huAiYp-An"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YHz2D-oIqBWa"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LcC1AwjoqfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e839f2c2-118f-42bb-afff-91511f324a96"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow_federated\n",
        "!pip install --quiet --upgrade nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 320.4MB 47kB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 67.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 55.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 57.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 58.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 54.9MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjDQysatrc2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb49e9e-7360-4058-d2f6-11063338f376"
      },
      "source": [
        "import collections\n",
        "import io\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Test that TFF is working:\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kVH5vzzNb2D"
      },
      "source": [
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def norun_except_tests(*args, **kwargs):\n",
        "    return"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYscs62B11T0"
      },
      "source": [
        "**NOTE**: This colab has been verified to work with the [latest released version](https://github.com/tensorflow/federated#compatibility) of the `tensorflow_federated` pip package, but the Tensorflow Federated project is still in pre-release development and may not work on `main`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0AwBLnk19UB"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-nW6M0c2CWa"
      },
      "source": [
        "In the [image classification](federated_learning_for_image_classification.ipynb) and\n",
        "[text generation](federated_learning_for_text_generation.ipynb) tutorials, we learned how to set up model and data pipelines for Federated Learning using datasets provided by TFF.\n",
        "\n",
        "In order to try Federated Learning for different applications, you may want to provide your own dataset. This tutorial shows you how to load a CSV file into a `tff.simulation.ClientData` for use in federated computations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKMUn-TlgxuP"
      },
      "source": [
        "## Creating and Downloading a CSV File with Shakespeare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXUOpI4b66I6"
      },
      "source": [
        "Before we can demonstrate loading a CSV file for use in TFF, we need to create two CSV files: one with training data, and one with testing data. We will load a Shakespeare dataset from the `tff.simulation.datasets` package and convert the data into CSV format. This is the same dataset used in the [text generation](federated_learning_for_text_generation.ipynb) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGqm_EJp7DjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cdea20e-e8e1-4ae5-c059-70daa5926e84"
      },
      "source": [
        "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/shakespeare.tar.bz2\n",
            "1851392/1848122 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No1Nymza7MD2"
      },
      "source": [
        "The TFF dataset is partitioned by client ID, where each client corresponds to a dataset on a particular device that might participate in federated learning. In the case of the Shakespeare dataset, each client is a character from Shakespeare, and the `client_id` is a character's name.\n",
        "\n",
        "To get a `tf.data.Dataset` for a particular client, we can use the `create_tf_dataset_for_client` function. In the case of this dataset, each `tf.data.Dataset` consists of multiple lines (`snippets`) spoken by that Shakespeare character. We create a column in the CSV file for each snippet, with the `client_id` in the `character` column. Thus, the same character's name can appear in many rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFCX0fz68BbB"
      },
      "source": [
        "def write_data_to_csv_file(tff_dataset, f):\n",
        "  f.write('\"character\",\"snippets\"\\n')\n",
        "  # Use a subset of the clients to speed up execution.\n",
        "  for client_id in tff_dataset.client_ids[200:]:\n",
        "    tf_dataset = tff_dataset.create_tf_dataset_for_client(client_id)\n",
        "    for element in tf_dataset.as_numpy_iterator():\n",
        "      # The CSV standard specifies that double quotes in the data must be escaped by preceding them with another double quote.\n",
        "      f.write('\"' + client_id + '\",\"' + str(element['snippets'], 'ascii').replace('\"', '\"\"') + '\"\\n')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR2DQTrc6JS1"
      },
      "source": [
        "We will create two separate files, one for training data and one for testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXwspNPnHwGl"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "filenames = ['shakespeare_train.csv', 'shakespeare_test.csv']\n",
        "for filename, data in zip(filenames, [train_data, test_data]):\n",
        "  with open(filename, 'w') as f:\n",
        "    write_data_to_csv_file(data, f)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4nDR1nMihz"
      },
      "source": [
        "Let's see what the first few lines of each file look like. The client keys consist of the name of the play joined with\n",
        "the name of the character, so for example `MUCH_ADO_ABOUT_NOTHING_OTHELLO` corresponds to the lines for the character Othello in the play *Much Ado About Nothing*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ZuDrv9X1a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6901996-cc00-4e70-8f74-b64645823d22"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "for filename in filenames:\n",
        "  with open(filename, 'r') as f:\n",
        "    print(\"Reading file \" + filename)\n",
        "    for i in range(10):\n",
        "      print(f.readline())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file shakespeare_train.csv\n",
            "\"character\",\"snippets\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_EXTON\",\"Both have I spill'd. O, would the deed were good!\n",
            "\n",
            "For now the devil, that told me I did well,\n",
            "\n",
            "Says that this deed is chronicled in hell.\n",
            "\n",
            "This dead King to the living King I'll bear.\n",
            "\n",
            "Take hence the rest, and give them burial here.       Exeunt\n",
            "\n",
            "Great King, within this coffin I present\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_EXTON\",\"'Have I no friend will rid me of this living fear?'\n",
            "\n",
            "Was it not so?\n",
            "\n",
            "'Have I no friend?' quoth he. He spake it twice\"\n",
            "\n",
            "Reading file shakespeare_test.csv\n",
            "\"character\",\"snippets\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_EXTON\",\"Didst thou not mark the King, what words he spake?\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_FIRST_CITIZEN\",\"Give you good morrow, sir.\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_FIRST_CITIZEN\",\"mother.\n",
            "\n",
            "Come, come, we fear the worst; all will be\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_FIRST_HERALD\",\"Harry of Hereford, Lancaster, and Derby,\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_FIRST_MURDERER\",\"done.\n",
            "\n",
            "Where's thy conscience now?\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_FIRST_MURDERER\",\"We are, my lord, and come to have the\"\n",
            "\n",
            "\"PERICLES__PRINCE_OF_TYRE_FIRST_MURDERER\",\"persuading me not to kill the Duke.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8JbRYaO6jQY"
      },
      "source": [
        "Now we can download the files we just created. You may need to click \"Allow\" on a a popup in your browser asking for permission to download the files from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCp8rMSOX54K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bd7d8b55-3a53-4285-81a7-4555736aac62"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "for filename in filenames:\n",
        "  files.download(filename)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_733c8a46-d9b3-42dc-a83a-2b01005f0717\", \"shakespeare_train.csv\", 2181298)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1cff1b2d-0db2-43bf-a210-c688a640824f\", \"shakespeare_test.csv\", 320054)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9P8VR5O606x"
      },
      "source": [
        "Take a look in your downloads folder to ensure that the files `shakespeare_train.csv` and `shakespeare_test.csv` were downloaded. Now we can delete the files from colab, since we will be uploading them from our local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhonzrXY7nFG"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "!rm \"shakespeare_train.csv\"\n",
        "!rm \"shakespeare_test.csv\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeGid47H651j"
      },
      "source": [
        "## Uploading files into Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6dBxARy78P2"
      },
      "source": [
        "Now we can upload the CSV files, as if we had the data in CSV format locally in the first place. The `files.upload()` function should bring up a \"Choose Files\" button. When you click this button, you should be able to choose files from your filesystem to upload. Choose the `shakespeare_train.csv` and `shakespeare_test.csv` files that you downloaded in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqHSXIFBLDc1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d6cc0874-d174-460e-8c1c-788c9fe7ebdb"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-db809f91-9255-40d4-9516-f44072e73e43\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-db809f91-9255-40d4-9516-f44072e73e43\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving shakespeare_train.csv to shakespeare_train.csv\n",
            "Saving shakespeare_test.csv to shakespeare_test.csv\n",
            "User uploaded file \"shakespeare_train.csv\" with length 2181298 bytes\n",
            "User uploaded file \"shakespeare_test.csv\" with length 320054 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eny3lDdM8oja"
      },
      "source": [
        "## Converting CSV Data to a TFF Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iiY65Vv4QNK"
      },
      "source": [
        "We will now write a function that returns a federated dataset (`tff.simulation.ClientData`) given the CSV file contents. \n",
        "\n",
        "We will use Pandas to read in the CSV data. Then we implement `create_tf_dataset_for_client_fn`, which takes a client ID and returns a TensorFlow dataset for that client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KAkKA4DDcBy"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "# Create Pandas dataframes from the CSV files.\n",
        "train_load_csv = pd.read_csv(io.StringIO(uploaded['shakespeare_train.csv'].decode(\"ascii\")))\n",
        "test_load_csv = pd.read_csv(io.StringIO(uploaded['shakespeare_test.csv'].decode(\"ascii\")))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7EFLqc4NgRU"
      },
      "source": [
        "# Running this cell will call the write_data_to_csv_file\n",
        "# function to write the data from the Shakespeare datasets\n",
        "# to in-memory buffers, and then load the contents of\n",
        "# the buffers into Pandas dataframes.\n",
        "# If the previous cell executed successfully, there is\n",
        "# no need to run this one.\n",
        "# If you would like to skip the steps above involving\n",
        "# downloading and re-uploading the CSV files,\n",
        "# you can comment out the line below to run this cell.\n",
        "%%norun_except_tests\n",
        "\n",
        "train_stream = io.StringIO()\n",
        "write_data_to_csv_file(train_data, train_stream)\n",
        "train_stream.seek(0)\n",
        "train_load_csv = pd.read_csv(train_stream)\n",
        "\n",
        "test_stream = io.StringIO()\n",
        "write_data_to_csv_file(test_data, test_stream)\n",
        "test_stream.seek(0)\n",
        "test_load_csv = pd.read_csv(io.StringIO(test_stream.getvalue()))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oadEzMrp-R6O"
      },
      "source": [
        "client_id_colname = 'character' # the column that represents client ID\n",
        "# The signature of the `tf.data.Dataset` that will be output for each client.\n",
        "output_signature = {'snippets': tf.TensorSpec(shape=(), dtype=tf.string)}\n",
        "# Rows for which any of the columns in this list are null will be discarded.\n",
        "notnull_cols = output_signature.keys()\n",
        "\n",
        "def create_tff_dataset_for_csv_file(df):\n",
        "  # Collect unique character names.\n",
        "  client_ids = df[client_id_colname].unique().tolist()\n",
        "\n",
        "  # Define a function that takes client ID and returns a tf.data.Dataset for\n",
        "  # that client. The tf.data.Dataset should contain a dictionary for each\n",
        "  # line spoken by the character, where the single key in each dictionary is\n",
        "  # \"snippets\" and the value is a string tensor containing the text of the line.\n",
        "  def create_tf_dataset_for_client_fn(client_id):\n",
        "    # Retrieve only the rows corresponding to this client.\n",
        "    client_data = df[df[client_id_colname] == client_id]\n",
        "    # Filter out any rows without a snippet of text spoken by the character.\n",
        "    client_data = client_data[client_data[notnull_cols].notnull().all(axis=1)]\n",
        "    # Select the data columns, discarding the client id column.\n",
        "    client_data = client_data[output_signature.keys()]\n",
        "    # Convert to a dictionary in the format\n",
        "    # [{column1 : value1, column2 : value2}]\n",
        "    records = client_data.to_dict('records')\n",
        "\n",
        "    # Define a generator that outputs a map for each row with column names as\n",
        "    # keys and row contents as values. In this example there is only one column,\n",
        "    # 'snippets', but this approach is shown to demonstrate how one might\n",
        "    # load a CSV file with more columns.\n",
        "    def dataset_gen():\n",
        "      for row in records:\n",
        "        yield row\n",
        "    # Generate a dataset for the client, specifying the output type explicitly\n",
        "    # as otherwise Tensorflow expects a tensor as the toplevel type.\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        dataset_gen,\n",
        "        output_types={k:v.dtype for k,v in output_signature.items()},\n",
        "        output_shapes={k:v.shape for k,v in output_signature.items()}\n",
        "    )\n",
        "\n",
        "  # Now that we have a list of client IDs and a function to generate a dataset\n",
        "  # for each client, we can use from_clients_and_fn to create the federated\n",
        "  # dataset.\n",
        "  return tff.simulation.ClientData.from_clients_and_fn(\n",
        "      client_ids=client_ids,\n",
        "      create_tf_dataset_for_client_fn=create_tf_dataset_for_client_fn\n",
        "  )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZfXT1yIPmMn"
      },
      "source": [
        "test_data = create_tff_dataset_for_csv_file(test_load_csv)\n",
        "train_data = create_tff_dataset_for_csv_file(train_load_csv)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQeHlqtozxOS"
      },
      "source": [
        "The datasets we just created consist of a sequence of maps from the key 'snippet' to \n",
        "string `Tensors`, one for each line spoken by a particular character in a\n",
        "Shakespeare play.\n",
        "\n",
        "We can get the data for a particular client by calling `create_tf_dataset_for_client` with that client_id. Note that in a real federated learning scenario\n",
        "clients are never identified or tracked by ids, but for simulation it is useful\n",
        "to work with keyed datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2wV12H9-n6Q"
      },
      "source": [
        "Let's take a look at lines spoken by Exton from *Pericles, Prince of Tyre*. We saw these lines printed above in the CSV file, so we can make sure they were loaded faithfully into the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a2oucKB-mm2",
        "outputId": "599aad1e-7dec-47f1-9e4c-94603072600d"
      },
      "source": [
        "# Here the play is \"Pericles, Prince of Tyre\" and the character is \"Exton\".\n",
        "raw_example_train_dataset = train_data.create_tf_dataset_for_client(\n",
        "    'PERICLES__PRINCE_OF_TYRE_EXTON')\n",
        "# Each entry x is a dictionary with a single key 'snippets' which contains the\n",
        "# text. If you import your own dataset, you may have more keys in each\n",
        "# dictionary corresponding to different features.\n",
        "for x in raw_example_train_dataset.take(2):\n",
        "  print(x)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'snippets': <tf.Tensor: shape=(), dtype=string, numpy=b\"Both have I spill'd. O, would the deed were good!\\nFor now the devil, that told me I did well,\\nSays that this deed is chronicled in hell.\\nThis dead King to the living King I'll bear.\\nTake hence the rest, and give them burial here.       Exeunt\\nGreat King, within this coffin I present\">}\n",
            "{'snippets': <tf.Tensor: shape=(), dtype=string, numpy=b\"'Have I no friend will rid me of this living fear?'\\nWas it not so?\\n'Have I no friend?' quoth he. He spake it twice\">}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRrXTF1I_DT_"
      },
      "source": [
        "To make sure the test data loaded correctly, we can look at some data from King Lear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEKiy1ntmmnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88650ce6-23a6-43e9-cbcf-fad9af8e229d"
      },
      "source": [
        "# Here the play is \"The Tragedy of King Lear\" and the character is \"King\".\n",
        "raw_example_test_dataset = test_data.create_tf_dataset_for_client(\n",
        "    'THE_TRAGEDY_OF_KING_LEAR_KING')\n",
        "# Each entry x is a dictionary with a single key 'snippets' which contains the\n",
        "# text. If you import your own dataset, you may have more keys in each\n",
        "# dictionary corresponding to different features.\n",
        "for x in raw_example_test_dataset.take(2):\n",
        "  print(x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'snippets': <tf.Tensor: shape=(), dtype=string, numpy=b'Sir, I will pronounce your sentence: you shall fast a week'>}\n",
            "{'snippets': <tf.Tensor: shape=(), dtype=string, numpy=b'Teach us, sweet madam, for our rude transgression'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-mqwhReCcBi"
      },
      "source": [
        "Now that the data from the CSV file has been loaded into a `tff.simulation.ClientData`, we can use `tf.data.Dataset` transformations to prepare the data for training. Refer to the [text generation](federated_learning_for_text_generation.ipynb) tutorial for instructions on how to transform the data and train a model using Federated Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8BxCwTKCw_B"
      },
      "source": [
        "## Modifying the Code to Work With Your Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93rniObdC-BY"
      },
      "source": [
        "If you have a CSV file you would like to use for Federated Learning, you can try modifying the code above to load your file into a `tff.simulation.ClientData`. The code was written to be easily modifiable to work with different datasets, but there are a few things you will need to consider:\n",
        "\n",
        "* `client_id_colname`: What column from your CSV file will be used as the `client_id`? This is a fundamental question about how you want to partition your data into client datasets. For realistic simulation of the challenges of Federated Learning, you may *not* want your data to be independent and identically distributed (IID) across clients.\n",
        "\n",
        "* `output_signature`: What is the type of the elements of the `tf.data.Dataset` that will be generated for each client_id? In this example, we had only one data column, `snippets`, but your dataset may have multiple data columns corresponding to different features and labels, possibly with different types.\n",
        "\n",
        "* `notnull_cols`: The current implementation will filter out rows in which any of the data columns is null. However, you might want to change this if null values are tolerable for some of the columns. Are there other conditions you want to filter on, such as filtering out rows with NaN values? \n"
      ]
    }
  ]
}
