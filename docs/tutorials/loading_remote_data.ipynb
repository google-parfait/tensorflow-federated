{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqjnBbmi8BPM"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PTdfUcwp8Eru"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_JWOQsjxDDo"
      },
      "source": [
        "# Loading Remote Data in TFF\n",
        "\n",
        "---\n",
        "\n",
        "**NOTE**: This colab has been verified to work with the [latest released version](https://github.com/tensorflow/federated#compatibility) of the `tensorflow_federated` pip package, but the Tensorflow Federated project is still in pre-release development and may not work on `main`.\n",
        "\n",
        "In a real-world application of federated learning, the raw training data is typically distributed across many devices or data silos -- requiring special preprocessing and loading before it's usable.\n",
        "\n",
        "This tutorial describes how to load examples stored in those remote locations with TFF's `DataBackend` and `DataExecutor` interfaces, and use them to train a model using federated learning. We'll demonstrate the use of data loading APIs by using a training dataset stored locally and simulate the sampling of examples as if the dataset were partitioned over distinct remote clients. As you adapt this tutorial to your use case, you will simply swap that dataset with your own distributed data.\n",
        "\n",
        "If you're new to federated learning or TFF, consider reading [Federated Learning for Image Classification](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) for a primer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNJf48UE8V-f"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/loading_remote_data\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.45.0/docs/tutorials/loading_remote_data.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.45.0/docs/tutorials/loading_remote_data.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/federated/docs/tutorials/loading_remote_data.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sxoeaXPC8LF"
      },
      "source": [
        "## Before we start\n",
        "\n",
        "Before we start, please run the following to make sure that your environment is\n",
        "correctly setup. Refer to the\n",
        "[Installation](../install.md) guide for more information. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrGitA_KnRO0"
      },
      "outputs": [],
      "source": [
        "#@title Set up open-source environment\n",
        "#@test {\"skip\": true}\n",
        "\n",
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [],
      "source": [
        "#@title Import packages\n",
        "import collections\n",
        "import random\n",
        "from typing import Any, List, Optional, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIFgAEZID7_X"
      },
      "source": [
        "## Preparing the input data\n",
        "\n",
        "Let's begin by loading TFF's federated version of the [EMNIST dataset](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification#preparing_the_input_data) from the built-in repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "och-dTLZoPHI"
      },
      "outputs": [],
      "source": [
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xkyMrciFYJc"
      },
      "source": [
        "And construct a preprocessing function to transform the raw examples in the EMNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kyiOMDO2GXKU"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 5\n",
        "SHUFFLE_BUFFER = 100\n",
        "\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def map_fn(element):\n",
        "    # Rename the features from `pixels` and `label`, to `x` and `y` for use with\n",
        "    # Keras.\n",
        "    return collections.OrderedDict(\n",
        "        # Transform each `28x28` image into a `784`-element array.\n",
        "        x=tf.reshape(element['pixels'], [-1, 784]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  # Shuffle the individual examples and `repeat` over several epochs.\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).map(map_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0prl3BhGWHm"
      },
      "source": [
        "Let's verify this works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LTV06PafG0zV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003cMapDataset element_spec=OrderedDict([('x', TensorSpec(shape=(1, 784), dtype=tf.float32, name=None)), ('y', TensorSpec(shape=(1, 1), dtype=tf.int32, name=None))])\u003e\n"
          ]
        }
      ],
      "source": [
        "# The local dataset corresponding to a single client as tf.data.Dataset.\n",
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0])\n",
        "\n",
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "print(preprocessed_example_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtnMawhie2Nh"
      },
      "source": [
        "Next, we'll construct an implementation of `DataBackend` that will load and preprocess the local examples from clients in the EMNIST dataset, which is crucial for fetching trainable examples during federated learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n-grt-mHIYG"
      },
      "source": [
        "## Defining how to fetch client data\n",
        "\n",
        "We need an instance of `DataBackend` to instruct the TFF workers how to load and tranform the local data.\n",
        "\n",
        "TFF workers are the processes that run on edge machines and perform the work for a single or multiple logical clients. In this example, the EMNIST dataset we'll use for training is already partitioned by logical clients and all the workers are going to be running in the same local environment. So our `DataBackend` can reference the data corresponding to any client. But in a non-experimental setting, the TFF workers will be distributed over individual remote machines, each mapping to a distinct set of clients, and you need to ensure that the `DataBackend` can correctly resolve data references according to its local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uNFPMzCxl-4A"
      },
      "outputs": [],
      "source": [
        "# A `DataBackend` is a programmatic construct that resolves symbolic references,\n",
        "# represented as application-specific URIs, to materialized examples that\n",
        "# TFF operations can process.\n",
        "class MyDataBackend(tff.framework.DataBackend):\n",
        "\n",
        "  async def materialize(self, data, type_spec):\n",
        "    # In this example, the URI contains the Id of a client.\n",
        "    client_id = int(data.uri[-1])\n",
        "    # The client Id is used to retrieve the corresponding local data.\n",
        "    client_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "        emnist_train.client_ids[client_id])\n",
        "    # We process the client dataset before returning so its compatible with our\n",
        "    # model definitions.\n",
        "    return preprocess(client_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10G3IcFLl9QC"
      },
      "source": [
        "## Setting up the runtime environment\n",
        "\n",
        "TFF computations are invoked by an `ExecutionContext` and in order for data URIs defined in TFF computations to be understood at runtime, a custom context must be defined for TFF workers that includes a pointer to the `DataBackend` we just created, so URIs can be properly resolved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OAKm2SxHnIA6"
      },
      "outputs": [],
      "source": [
        "def ex_fn(device: tf.config.LogicalDevice) -\u003e tff.framework.DataExecutor:\n",
        "  # A `DataBackend` object is wrapped by a `DataExecutor`, which queries the\n",
        "  # backend when a TFF worker encounters an operation requires fetching local\n",
        "  # data.\n",
        "  return tff.framework.DataExecutor(\n",
        "      tff.framework.EagerTFExecutor(device), data_backend=MyDataBackend())\n",
        "\n",
        "\n",
        "# In a distributed setting, this needs to run in the TFF worker as a service\n",
        "# connecting to some port. The top-level controller feeding TFF computations\n",
        "# would then connect to this port.\n",
        "factory = tff.framework.local_executor_factory(leaf_executor_fn=ex_fn)\n",
        "ctx = tff.framework.SyncExecutionContext(executor_fn=factory)\n",
        "tff.framework.set_default_context(ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDPVBPsnKcg"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now we are ready to train a model using federated learning. Lets define a Keras model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "X_iXlL6corkR"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(784,)),\n",
        "      tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "      tf.keras.layers.Softmax(),\n",
        "  ])\n",
        "\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBfrmkq_pDd0"
      },
      "source": [
        "We can pass this TFF-wrapped definition of our model\n",
        "to a [Federated Averaging](https://arxiv.org/abs/1602.05629) algorithm by invoking the helper\n",
        "function `tff.learning.algorithms.build_weighted_fed_avg`, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SGORt8ImmcLH"
      },
      "outputs": [],
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "state = iterative_process.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT2r5Ayhpoaa"
      },
      "source": [
        "The `initialize` computation returns the initial state of the\n",
        "Federated Averaging process.\n",
        "\n",
        "To run a round of training, we need to construct a sample of data by collecting a sample of URI references as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f2igGrhfpmqY"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 10\n",
        "\n",
        "element_type = tff.types.StructWithPythonType(\n",
        "    preprocessed_example_dataset.element_spec,\n",
        "    container_type=collections.OrderedDict)\n",
        "dataset_type = tff.types.SequenceType(element_type)\n",
        "\n",
        "round_data_uris = [f'uri://{i}' for i in range(NUM_CLIENTS)]\n",
        "round_train_data = tff.framework.CreateDataDescriptor(\n",
        "    arg_uris=round_data_uris, arg_type=dataset_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Pd90E206FT"
      },
      "source": [
        "Now we can round a round of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2nOmtKF7p4Vo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.11234568), ('loss', 11.965633), ('num_examples', 4860), ('num_batches', 4860)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
          ]
        }
      ],
      "source": [
        "result = iterative_process.next(state, round_train_data)\n",
        "state = result.state\n",
        "metrics = result.metrics\n",
        "print('round 1, metrics={}'.format(metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIrJ4a5aqD6D"
      },
      "source": [
        "### Training over multiple rounds\n",
        "\n",
        "We can define a `FederatedDataSource` container for selecting clients and assembling the inputs for retrieving local data. This makes it convenient to loop over multiple rounds of training, and can be reused across multiple training jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "S8ZNcpnZN0IA"
      },
      "outputs": [],
      "source": [
        "class MyFederatedDataSourceIterator(tff.program.FederatedDataSourceIterator):\n",
        "\n",
        "  def __init__(self, client_ids: Sequence[str],\n",
        "               federated_type: tff.FederatedType):\n",
        "    self._client_ids = client_ids\n",
        "    self._federated_type = federated_type\n",
        "\n",
        "  @property\n",
        "  def federated_type(self) -\u003e tff.FederatedType:\n",
        "    return self._federated_type\n",
        "\n",
        "  def select(self, num_clients: Optional[int] = None) -\u003e Any:\n",
        "    client_ids_sample = random.sample(self._client_ids, num_clients)\n",
        "    data_uris = [f'uri://{i}' for i in client_ids_sample]\n",
        "    return tff.framework.CreateDataDescriptor(\n",
        "        arg_uris=data_uris, arg_type=self._federated_type)\n",
        "\n",
        "\n",
        "class MyFederatedDataSource(tff.program.FederatedDataSource):\n",
        "\n",
        "  def __init__(self, client_ids: Sequence[str],\n",
        "               federated_type: tff.FederatedType):\n",
        "    self._client_ids = client_ids\n",
        "    self._federated_type = federated_type\n",
        "    self._capabilities = [tff.program.Capability.RANDOM_UNIFORM]\n",
        "\n",
        "  @property\n",
        "  def federated_type(self) -\u003e tff.FederatedType:\n",
        "    return self._federated_type\n",
        "\n",
        "  @property\n",
        "  def capabilities(self) -\u003e List[tff.program.Capability]:\n",
        "    return self._capabilities\n",
        "\n",
        "  def iterator(self) -\u003e tff.program.FederatedDataSourceIterator:\n",
        "    return MyFederatedDataSourceIterator(self._client_ids, self._federated_type)\n",
        "\n",
        "\n",
        "train_data_source = MyFederatedDataSource(\n",
        "    client_ids=emnist_train.client_ids, federated_type=dataset_type)\n",
        "train_data_iterator = train_data_source.iterator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxRz3rNwPHCV"
      },
      "source": [
        "Now we can run our federated learning training loop like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Zik5RUTWYQBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.12357217), ('loss', 9.161968), ('num_examples', 4815), ('num_batches', 4815)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.20563674), ('loss', 7.0862083), ('num_examples', 4790), ('num_batches', 4790)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.30241227), ('loss', 5.6945825), ('num_examples', 4560), ('num_batches', 4560)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.3867347), ('loss', 4.7210026), ('num_examples', 4900), ('num_batches', 4900)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round  6, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.42311886), ('loss', 4.205554), ('num_examples', 4585), ('num_batches', 4585)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round  7, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.4501548), ('loss', 4.1297464), ('num_examples', 4845), ('num_batches', 4845)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round  8, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.56590474), ('loss', 2.8927681), ('num_examples', 5250), ('num_batches', 5250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round  9, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.59917355), ('loss', 2.7431731), ('num_examples', 4840), ('num_batches', 4840)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "round 10, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.5717234), ('loss', 2.9738288), ('num_examples', 4845), ('num_batches', 4845)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
          ]
        }
      ],
      "source": [
        "NUM_ROUNDS = 10\n",
        "\n",
        "for round_num in range(2, NUM_ROUNDS + 1):\n",
        "  round_train_data = train_data_iterator.select(NUM_CLIENTS)\n",
        "  result = iterative_process.next(state, round_train_data)\n",
        "  state = result.state\n",
        "  metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itjHBjJ6qOoS"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This concludes the tutorial. We encourage you to explore the other tutorials we've developed to learn about the many other features of the TFF framework."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "loading_remote_data.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
