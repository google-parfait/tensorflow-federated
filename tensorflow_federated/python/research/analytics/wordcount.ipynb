{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_lNeCgAVkdhM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "uDcWxmG9kh1Q"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UhoPXG8g1CBt"
      },
      "source": [
        "# WordCount in TFF\n",
        "\n",
        "This notebook demonstrates a basic analytics query (count the top occurrences of each word in Shakespeare), implemented first as pure-Tensorflow and then as a TFF computation.\n",
        "\n",
        "The goal is to demonstrate an analytics query in TFFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ary-OZz5jMJI"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
        "# pip package, you may need to edit the following to point to the '.whl' file\n",
        "# on your local filesystem.\n",
        "\n",
        "!pip install --quiet tensorflow_federated\n",
        "!pip install --quiet tf-nightly==1.15.0.dev20190805\n",
        "!pip install --quiet tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "f6JYwcvYBkGl"
      },
      "outputs": [],
      "source": [
        "import tensorflow.google as tf\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow_text as text\n",
        "\n",
        "from tensorflow_datasets.core.features import text as tfds_text\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GPbOsTO5Bwu1"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/shakespeare/load_data\n",
        "shake_train, shake_test = tff.simulation.datasets.shakespeare.load_data()\n",
        "\n",
        "training_ds = shake_train.create_tf_dataset_from_all_clients()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J3uvbmRCB0QA"
      },
      "source": [
        "# Dataset Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ik3DNHAc1b7j"
      },
      "outputs": [],
      "source": [
        "## Preprocess the dataset to split each line into individual words\n",
        "\n",
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "def preprocess(ds):\n",
        "  ds = ds.map(lambda l: l['snippets'])\n",
        "  ds = ds.flat_map(lambda l: tf.data.Dataset.from_tensor_slices(\n",
        "      tokenizer.tokenize(l)[0]))\n",
        "  ds = ds.map(text.case_fold_utf8)\n",
        "  ds = ds.filter(lambda w: tf.math.logical_not(text.wordshape(w, text.WordShape.IS_PUNCT_OR_SYMBOL)))\n",
        "  ds = ds.shuffle(buffer_size=50000)\n",
        "  return ds\n",
        " \n",
        "\n",
        "dataset = preprocess(training_ds)\n",
        "for i in dataset.take(3):\n",
        "  print(i.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iyv4sCQA9mgU"
      },
      "outputs": [],
      "source": [
        "## Build a vocab dictionary by getting the set of unique words\n",
        "\n",
        "vocab = dataset.apply(tf.data.experimental.unique())\n",
        "vocab_list = [t.numpy() for t in vocab]\n",
        "print('Final vocab length %d' % len(vocab_list))\n",
        "print(vocab_list[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UAGhFahDB7fm"
      },
      "source": [
        "# Pure Tensorflow implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uqk1HRZ2jUtO"
      },
      "outputs": [],
      "source": [
        "# BATCH_SIZE = 5\n",
        "# TAKE = 2\n",
        "# K = 5\n",
        "BATCH_SIZE = 10000\n",
        "TAKE = -1\n",
        "K = 30\n",
        "\n",
        "vocab_lookup = tf.lookup.index_table_from_tensor(vocab_list)\n",
        "vocab_size = tf.cast(vocab_lookup.size(), tf.int32)\n",
        "print('Vocab size: %d' % vocab_size.numpy())\n",
        "\n",
        "counts = tf.zeros([vocab_size])\n",
        "for batch in dataset.batch(BATCH_SIZE).take(TAKE):\n",
        "  indices = vocab_lookup.lookup(batch)\n",
        "  onehot = tf.one_hot(indices, depth=vocab_size)\n",
        "  counts += tf.reduce_sum(onehot, axis=0)\n",
        "  top_vals, top_indices = tf.math.top_k(counts, k=K)\n",
        "  top_words = tf.gather(vocab_list, top_indices)\n",
        "  print('.', end='')\n",
        "\n",
        "print()\n",
        "for word,count in zip(top_words, top_vals):\n",
        "  print('%s: %d' % (word.numpy().decode('utf-8'), count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2mqUEgAQCqjt"
      },
      "source": [
        "# Tensorflow Federated Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SGyFDlLx0K7e"
      },
      "outputs": [],
      "source": [
        "## Dataset prep\n",
        "\n",
        "client_ids = shake_train.client_ids\n",
        "client_datasets = [preprocess(shake_train.create_tf_dataset_for_client(id)) for id in client_ids]\n",
        "\n",
        "print('Num clients: %d' % len(client_datasets))\n",
        "for ds in client_datasets[:3]:\n",
        "  for words in ds.batch(3).take(1):\n",
        "    print(words.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PKdD2gW3Cz_9"
      },
      "outputs": [],
      "source": [
        "## Initial decomposition to map-reduce style, not actually TFF just yet!\n",
        "@tf.function\n",
        "def client_map_step(ds):\n",
        "  # N.B. vocab_size and vocab_lookup must be created inside the @tf.function\n",
        "  vocab_size = len(vocab_list)\n",
        "  vocab_lookup = tf.lookup.index_table_from_tensor(vocab_list)\n",
        "  \n",
        "  @tf.function\n",
        "  def _count_words_in_batch(acummulator, batch):    \n",
        "    indices = vocab_lookup.lookup(batch)\n",
        "    onehot = tf.one_hot(indices, depth=tf.cast(vocab_size, tf.int32), dtype=tf.int32)\n",
        "    return acummulator + tf.reduce_sum(onehot, axis=0)\n",
        "\n",
        "  return ds.batch(BATCH_SIZE).take(TAKE).reduce(\n",
        "      initial_state=tf.zeros([vocab_size], tf.int32),\n",
        "      reduce_func=_count_words_in_batch)\n",
        "\n",
        "@tf.function\n",
        "def cross_client_reduce_step(client_aggregates):\n",
        "  reduced = tf.math.add_n(client_aggregates)\n",
        "  top_vals, top_indices = tf.math.top_k(reduced, k=K)\n",
        "  top_words = tf.gather(vocab_list, top_indices)\n",
        "  return top_words, top_vals\n",
        "\n",
        "# Wire it all together\n",
        "client_sums = list()\n",
        "for client_ds in client_datasets:\n",
        "  print('.', end='')\n",
        "  client_sums.append(client_map_step(client_ds))\n",
        "top_words, top_counts = cross_client_reduce_step(client_sums)\n",
        "\n",
        "\n",
        "print()\n",
        "for word,count in zip(top_words, top_vals):\n",
        "  print('%s: %d' % (word.numpy().decode('utf-8'), count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Qg53eovx3kZQ"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation(\n",
        "  tff.FederatedType((tff.SequenceType(tf.string)), tff.CLIENTS))\n",
        "def federated_top_k_words(client_datasets):\n",
        "  tff_map = tff.tf_computation(\n",
        "      client_map_step, client_datasets.type_signature.member)\n",
        "  print(tff_map.type_signature)  # (string* -\u003e int32[VOCAB_SIZE])\n",
        "\n",
        "  client_aggregates = tff.federated_map(tff_map, client_datasets)\n",
        "  print(client_aggregates.type_signature)  # {int32[VOCAB_SIZE]@CLIENTS}\n",
        "\n",
        "  @tff.tf_computation()\n",
        "  def build_zeros():\n",
        "    return tf.zeros([len(vocab_list)], tf.int32)\n",
        "  print(build_zeros.type_signature)  # ( -\u003e int32[VOCAB_SIZE])\n",
        "\n",
        "  @tff.tf_computation(tff_map.type_signature.result,\n",
        "                      tff_map.type_signature.result)\n",
        "  def accumulate(accum, delta):\n",
        "    return accum + delta\n",
        "  print(accumulate.type_signature)  # (\u003cint32[VOCAB_SIZE],int32[VOCAB_SIZE]\u003e -\u003e int32[VOCAB_SIZE])\n",
        "\n",
        "  @tff.tf_computation(accumulate.type_signature.result)\n",
        "  def report(accum):\n",
        "    top_vals, top_indices = tf.math.top_k(accum, k=K)\n",
        "    top_words = tf.gather(vocab_list, top_indices)\n",
        "    return top_words, top_vals\n",
        "  print(report.type_signature)  # (int32[VOCAB_SIZE] -\u003e \u003cstring[K],int32[]\u003e)\n",
        "\n",
        "  aggregate = tff.federated_aggregate(\n",
        "      value=client_aggregates,\n",
        "      zero=build_zeros(),\n",
        "      accumulate=accumulate,\n",
        "      merge=accumulate,\n",
        "      report=report,\n",
        "  )\n",
        "  print(aggregate.type_signature)  # \u003cstring[K],int32[K]\u003e@SERVER\n",
        "\n",
        "  return aggregate \n",
        "\n",
        "print(federated_top_k_words.type_signature)  # ({string*}@CLIENTS -\u003e \u003cstring[K],int32[K]\u003e@SERVER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C2C2xhR86MkI"
      },
      "outputs": [],
      "source": [
        "# NUM_CLIENTS = 64\n",
        "NUM_CLIENTS = None\n",
        "\n",
        "top_words, top_vals = federated_top_k_words([ds for ds in client_datasets[:NUM_CLIENTS]])\n",
        "for word,count in zip(top_words, top_vals):\n",
        "  print('%s: %d' % (word.decode('utf-8'), count))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "WordCount in TFF",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
